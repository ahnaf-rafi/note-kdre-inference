% ! TEX note = ../root-kdre-inference.tex

\section{Density Estimation}

\begin{assumption}
\label{asm--kde-dgp}
\(X, X_{1}, \dots, X_{n}\) are iid random \(\mathbb{R}^{d}\)-vectors all with
Lebesgue density \(f\).
\end{assumption}

\begin{assumption}
\label{asm--kernel}
\(K : \mathbb{R}^{d} \to \mathbb{R}\) is a measurable function such that
\begin{enumerate}[label=(\roman*)]
  \item
    \label{asm--kernel-integrability}
    \(\int |K (u)|^{p} \; \mathrm{d} u < \infty\) for each \(p \in \{1, 2,
    4\}\).
  \item
    \label{asm--kernel-unit-integral}
    \(\int K (u) \; \mathrm{d} u = 1\)
  \item \label{asm--kernel-thin-tails}
    \(\lim_{\|u\| \to \infty} \|u\|^{d} K (u) = 0\).
\end{enumerate}
\end{assumption}

\begin{assumption}
\label{asm--bandwidth}
\(\left\{ h_{n} \right\}\) is a real sequence such that \(h_{n} > 0\) for every
\(n \in \mathbb{N}\), \(\lim_{n \to \infty} h_{n} = 0\), and \(\lim_{n \to
\infty} 1 / \left( n h_{n}^{d} \right) = 0\).
\end{assumption}

\begin{remark}
It can be shown that if \(K\) is continuous, then
\Cref{asm--kernel} \ref{asm--kernel-integrability} for \(p = 1\) and
\ref{asm--kernel-thin-tails} imply that \(K\) is bounded.
Then \Cref{asm--kernel} \ref{asm--kernel-integrability} for \(p \in (1,
\infty)\) are immediately implied since \(|K (u)|^{p} \leq \left( \sup_{v \in
\mathbb{R}^{d}} |K (v)| \right)^{p - 1}
|K (u)|\) for every \(u \in \mathbb{R}^{d}\).
For our purposes, it will be sufficient to assume \Cref{asm--kernel}
\ref{asm--kernel-integrability} for \(p \in \{1, 2, 4\}\).
\end{remark}

For \(K\) satisfying \Cref{asm--kernel}, define
\begin{equation}
  K_{h} (u) := \frac{1}{h^{d}} K \left( \frac{u}{h} \right).
  \label{eqn--kernel-Kh}
\end{equation}
The kernel density estimator and its mean are
\begin{equation}
  \widehat{f}_{n, h} (x) = \frac{1}{n} \sum_{i = 1}^{n} K_{h} \left( X_{i} - x
  \right) \text{ and } f_{h} (x) = \E \left[ K_{h} (X - x) \right] = \int f (x +
  u) K_{h} (u) \; \mathrm{d} u.
  \label{eqn--kde-and-mean}
\end{equation}
Note that the final equality in \eqref{eqn--kde-and-mean} follows from the usual
change of variables formula.
The following is a usual statement of consistency and asymptotic normality for
\(\widehat{f}_{n, h_{n}} (x)\).

\begin{theorem}
\label{thm--kde-asymp-norm-usual}
Let the probability density \(f\) in \Cref{asm--kde-dgp} be continuous at \(x
\in \mathbb{R}^{d}\) and satisfy \(f (x) > 0\), let \(K\) be a function
satisfying \Cref{asm--kernel}, and let the sequence \(\left\{ h_{n} \right\}\)
satisfy \Cref{asm--bandwidth}.
Then as \(n \to \infty\),
\begin{equation}
  \widehat{f}_{n, h_{n}} (x) \overset{\mathrm{p}}{\to} f (x) \quad \text{and}
  \quad
  \sqrt{n h_{n}^{d}} \left( \widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x) \right)
  \rightsquigarrow \N \left( 0, f (x) \int K (u)^{2} \; \mathrm{d} u \right).
  \label{eqn--kde-asymp-norm-usual}
\end{equation}
\end{theorem}

\begin{proof}[Proof of \Cref{thm--kde-asymp-norm-usual}]
See \Cref{sec--prf--thm--kde-asymp-norm-usual}.
\end{proof}


In this note, we will avoid talking about asymptotic normality of
\(\widehat{f}_{n, h_{n}} - f (x)\) since the bias term \(f_{h_{n}} (x) - f (x)\)
is typically non-zero and requires slightly delicate handling after scaling by
\(\sqrt{n h_{n}^{d}}\).
Instead, we focus on the centered quantity \(\widehat{f}_{n, h_{n}} - f_{h_{n}}
(x)\) to focus only on asymptotic normality and standard error estimation.
To that end, the typical prescription for inference is based on an application
of Slutsky's Theorem and direct use of \eqref{eqn--kde-asymp-norm-usual}: under
the conditions of \Cref{thm--kde-asymp-norm-usual},
\begin{equation}
  \begin{gathered}
    \widetilde{t}_{n} = \frac{\sqrt{n h_{n}^{d}}\left( \widehat{f}_{n, h_{n}}
    (x) - f_{h_{n}} (x) \right)}{\sqrt{\widetilde{V}_{n, h_{n}} (x)}}
    \rightsquigarrow \N (0, 1), \\
    \text{where} \quad \widetilde{V}_{n, h_{n}} (x) = \widehat{f}_{n, h_{n}} (x)
    \int K (u)^{2} \; \mathrm{d} u.
  \end{gathered}
  \label{eqn--kde-asymp-norm-tstat-usual}
\end{equation}

In this note, we follow an alternative route.
In particular, \(\widehat{f}_{n, h}\) is a sample average and so,
we might suspect that usual \(t\)-statistic is asymptotically standard normal.
A key object in our derivations will be the centered kernel sum
\begin{equation}
  S_{n, h} (x) = \sum_{i = 1}^{n} \left\{ K \left( \frac{X_{i} - x}{h} \right) -
  \E \left[ K \left( \frac{X - x}{h} \right) \right] \right\}.
  \label{eqn--kernel-sum}
\end{equation}
Note that
\begin{equation}
  \widehat{f}_{n, h} (x) - f_{h} (x) = \frac{S_{n, h} (x)}{n h^{d}}.
  \label{eqn--fhat-diff-to-kernel-sum}
\end{equation}
Clearly since \(X_{i} \overset{\mathrm{iid}}{\sim} X\) (\Cref{asm--kde-dgp}),
\begin{equation}
  \E \left[ S_{n, h} (x) \right] = 0 \quad \text{and} \quad \mathrm{Var}
  \left[ S_{n, h} (x) \right] = n \Sigma_{h} (x),
  \label{eqn--kernel-sum-mean-and-var}
\end{equation}
where by \eqref{eqn--convl-kernel-eg-thin-tail} in
\Cref{thm--convl-kernel-eg-thin-tail},
\begin{equation}
  \begin{split}
    \Sigma_{h} (x) :=
    & \, \mathrm{Var} \left[ K \left( \frac{X - x}{h} \right) \right] = \E
    \left[ K \left( \frac{X - x}{h} \right)^{2} \right] - \E \left[ K \left(
    \frac{X - x}{h} \right) \right]^{2} \\
    =
    & \, h^{d} \int f (x + u h) K (u)^{2} \; \mathrm{d} u
    - \left( h^{d} \int f (x + u h) K (u) \; \mathrm{d} u \right)^{2}.
  \end{split}
  \label{eqn--Sigmah-expand}
\end{equation}
The following result shows that \(S_{n, h_{n}} (x) / \sqrt{n \Sigma_{h_{n}}
(x)}\) is asymptotically standard normal.
The key technical tool is Liapunov's Central Limit Theorem (see for example
\citet[Theorem 27.3, p. 362]{1995billingsleyProbabilityMeasure}
or \citet[Theorem 18 in Section 4 of Chapter III,
p. 51]{1984pollardConvergenceStochasticProcesses}).

\begin{theorem}
\label{thm--kernel-sum-asymp-norm}
Let the probability density \(f\) in \Cref{asm--kde-dgp} be continuous at \(x
\in \mathbb{R}^{d}\) and satisfy \(f (x) > 0\), let \(K\) be a function
satisfying \Cref{asm--kernel}, and let the sequence \(\left\{ h_{n} \right\}\)
satisfy \Cref{asm--bandwidth}.
Then as \(n \to \infty\),
\begin{equation}
  \frac{S_{n, h_{n}} (x)}{\sqrt{n \Sigma_{h_{n}} (x)}} \rightsquigarrow \N (0,
  1).
  \label{eqn--kernel-sum-asymp-norm}
\end{equation}
\end{theorem}

\begin{proof}[Proof of \Cref{thm--kernel-sum-asymp-norm}]
See \Cref{sec--prf--thm--kernel-sum-asymp-norm}.
\end{proof}

Define
\begin{equation*}
  \begin{split}
    V_{h} (x) = \Var \left[ K_{h} (X - x) \right] =
    & \, \E \left[ K_{h} (X - x)^{2} \right] - \E \left[ K_{h} (X - x)
    \right]^{2} \\
    =
    & \, \E \left[ K_{h} (X - x)^{2} \right] - f_{h} (x)^{2}.
  \end{split}
\end{equation*}
Note that
\begin{equation*}
  V_{h} (x) = \frac{1}{h^{2 d}} \Sigma_{h} (x).
\end{equation*}
Since \(\widehat{f}_{n, h_{n}}\) is a sample average of the \(K_{h} \left( X_{i}
- x \right)\)'s and \(X_{i} \overset{\mathrm{iid}}{\sim} X\), it follows that
\begin{equation*}
  \Var \left[ \widehat{f}_{n h} (x) \right] = V_{h} (x) / n.
\end{equation*}
Furthermore,
\begin{equation}
  \frac{\widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x)}{\sqrt{V_{h_{n}} (x) / n}} =
  \frac{S_{n, h_{n}} (x)}{\sqrt{n \Sigma_{h_{n}} (x)}} \rightsquigarrow \N (0,
  1),
  \label{eqn--kernel-sum-asymp-norm-1}
\end{equation}
by \eqref{eqn--kernel-sum-asymp-norm} in
\Cref{thm--kernel-sum-asymp-norm}.

For a feasible implementation of \eqref{eqn--kernel-sum-asymp-norm-1},
we require a consistent estimator (in a sense to be defined later on) of \(V_{h}
(x)\).
To that end, a reasonable estimator of \(V_{h} (x)\) is the sample variance of
the \(K_{h} \left( X_{i} - x \right)\)'s:
\begin{equation}
  \begin{split}
    \widehat{V}_{n, h} (x) =
    & \, \frac{1}{n} \sum_{i = 1}^{n} K_{h} \left( X_{i} - x \right)^{2} -
    \left( \frac{1}{n} \sum_{i = 1}^{n} K_{h} \left( X_{i} - x \right)
    \right)^{2} \\
    =
    & \, \frac{1}{n} \sum_{i = 1}^{n} K_{h} \left( X_{i} - x \right)^{2} -
    \widehat{f}_{n, h} (x)^{2}.
  \end{split}
\end{equation}
\Cref{thm--kde-asymp-norm-conv-var} shows that using \(\widehat{V}_{n, h}
(x)\) in standardization achieves asymptotic standard normality for the
resulting \(t\)-statistic.

\begin{theorem}
\label{thm--kde-asymp-norm-conv-var}
Let the probability density \(f\) in \Cref{asm--kde-dgp} be continuous at \(x
\in \mathbb{R}^{d}\) and satisfy \(f (x) > 0\), let \(K\) be a function
satisfying \Cref{asm--kernel}, and let the sequence \(\left\{ h_{n} \right\}\)
satisfy \Cref{asm--bandwidth}.
Then as \(n \to \infty\),
\begin{equation}
  \frac{\widehat{V}_{n, h_{n}} (x)}{V_{h_{n}} (x)} \overset{\mathrm{p}}{\to} 1
  \quad \text{and so} \quad \frac{\widehat{f}_{n, h_{n}} (x) - f_{h_{n}}
  (x)}{\sqrt{\widehat{V}_{n, h_{n}} (x) / n}} \rightsquigarrow \N (0, 1).
  \label{eqn--kde-asymp-norm-conv-var}
\end{equation}
\end{theorem}

\begin{proof}[Proof of \Cref{thm--kde-asymp-norm-conv-var}]
See \Cref{sec--prf--thm--kde-asymp-norm-conv-var}.
\end{proof}

\subsection{Proofs}

\subsubsection{Proof of
\texorpdfstring{\Cref{thm--kde-asymp-norm-usual}}{Theorem
\ref{thm--kde-asymp-norm-usual}}}
\label{sec--prf--thm--kde-asymp-norm-usual}

The consistency part of \eqref{eqn--kde-asymp-norm-usual} follows from the
asymptotic normality part of \eqref{eqn--kde-asymp-norm-usual} combined with
\Cref{thm--convl-kernel-eg-thin-tail}.
To see this, note that
\begin{equation*}
  \widehat{f}_{n, h_{n}} (x) - f (x) = \widehat{f}_{n, h_{n}} (x) - f_{h_{n}}
  (x) + f_{h_{n}} (x) - f (x),
\end{equation*}
and by \Cref{eqn--convl-kernel-eg-thin-tail} in
\Cref{thm--convl-kernel-eg-thin-tail},
\begin{equation*}
  \lim_{n \to \infty} f_{h_{n}} (x) = \lim_{n \to \infty} \int f \left( x + u
  h_{n} \right) K (u) \; \mathrm{d} u = f (x) \int K (u) \; \mathrm{d} u = f
  (x),
\end{equation*}
where the last equality follows from \Cref{asm--kernel}
\ref{asm--kernel-unit-integral}.
Hence, we are done if we can show \(\widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x)
\overset{\mathrm{p}}{\to} 0\).
But the asymptotic normality part of
\eqref{eqn--kde-asymp-norm-usual} implies that
\(\widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x) = O_{\mathrm{p}} \left( 1 / \sqrt{n
h_{n}^{d}} \right) = o_{\mathrm{p}} (1)\) since by \Cref{asm--bandwidth}, \(1 /
\left( n h_{n}^{d} \right) \to 0\) as \(n \to \infty\).

Hence, it remains to show the asymptotic normality part of
\eqref{eqn--kde-asymp-norm-usual}.
We shall derive the latter as a corollary of \Cref{thm--kernel-sum-asymp-norm}.
By \eqref{eqn--kernel-sum-asymp-norm} in \Cref{thm--kernel-sum-asymp-norm}
\begin{equation}
  Z_{n, h_{n}} (x) := \frac{S_{n, h_{n}} (x)}{\sqrt{n \Sigma_{h_{n}} (x)}}
  \rightsquigarrow \N (0, 1).
  \label{eqn--kde-asymp-norm-1}
\end{equation}
The definition of \(\Sigma_{h} (x)\) is given in \eqref{eqn--Sigmah-expand}.
By \eqref{eqn--fhat-diff-to-kernel-sum} and \eqref{eqn--kde-asymp-norm-1}
\begin{equation*}
  \widehat{f}_{n, h} (x) - f_{h} (x) = \frac{S_{n, h} (x)}{n h^{d}} =
  \frac{\sqrt{n \Sigma_{h} (x)}}{n h^{d}} \cdot Z_{n, h} (x) = \frac{\sqrt{h^{-
  d} \Sigma_{h} (x)}}{\sqrt{n h^{d}}} \cdot Z_{n, h} (x),
\end{equation*}
and so,
\begin{equation}
  \sqrt{n h^{d}} \left( \widehat{f}_{n, h} (x) - f_{h} (x) \right) =
  \sqrt{h^{- d} \Sigma_{h} (x)} \cdot Z_{n, h} (x).
  \label{eqn--fhat-diff-to-kernel-sum-1}
\end{equation}
Now by \eqref{eqn--Sigmah-expand},
\begin{align*}
  h^{- d} \Sigma_{h} (x) =
  & \, h^{- d} \left( h^{d} \int f (x + u h) K (u)^{2} \; \mathrm{d} u - \left(
  h^{d} \int f (x + u h) K (u) \; \mathrm{d} u \right)^{2} \right) \\
  =
  & \, \int f (x + u h) K (u)^{2} \; \mathrm{d} u - h^{d} \left( \int f (x + u
  h) K (u) \; \mathrm{d} u \right)^{2}.
\end{align*}
By \Cref{thm--convl-kernel-eg-thin-tail},
\begin{equation*}
  \lim_{n \to \infty} h_{n}^{- d} \Sigma_{h_{n}} (x) = f (x) \int K (u)^{2} \;
  \mathrm{d} u.
\end{equation*}
Combine the above display with \eqref{eqn--kde-asymp-norm-1},
\eqref{eqn--fhat-diff-to-kernel-sum-1}, and Slutsky's Theorem to get the
asymptotic normality part of \eqref{eqn--kde-asymp-norm-usual}.
\qed

\subsubsection{Proof of
\texorpdfstring{\Cref{thm--kernel-sum-asymp-norm}}{Theorem
\ref{thm--kernel-sum-asymp-norm}}}
\label{sec--prf--thm--kernel-sum-asymp-norm}

We use Liapunov's Central Limit Theorem to prove
\eqref{eqn--kernel-sum-asymp-norm}.
To that end, define
\begin{align*}
  b_{n, h} (x)^{4} =
  & \, \sum_{i = 1}^{n} \E \left[ \left| K \left( \frac{X_{i} - x}{h} \right) -
  \E \left[ K \left( \frac{X - x}{h} \right) \right] \right|^{4} \right] \\
  =
  & \, n \cdot \E \left[ \left| K \left( \frac{X - x}{h} \right) - \E \left[ K
  \left( \frac{X - x}{h} \right) \right] \right|^{4} \right].
\end{align*}
By Liapunov's Central Limit Theorem,
\begin{equation}
  \text{If} \quad \lim_{n \to \infty} \frac{b_{n, h_{n}} (x)^{4}}{\left( n
  \Sigma_{h_{n}} (x) \right)^{2}} = 0, \quad \text{then} \quad \frac{S_{n,
  h_{n}} (x)}{\sqrt{n \Sigma_{h_{n}} (x)}} \rightsquigarrow \N (0, 1).
  \label{eqn--liapunov-clt}
\end{equation}
By the \(C_{r}\) inequality (in particular \((|a| + |b|)^{4} \leq 8 \left(
|a|^{4} + |b|^{4} \right)\)),
\begin{equation*}
  b_{n, h} (x)^{4} \leq 8 n \cdot \left( \E \left[ K \left( \frac{X -
  x}{h} \right)^{4} \right] + \left| \E \left[ K \left( \frac{X - x}{h}
  \right) \right] \right|^{4} \right).
\end{equation*}
And so,
\begin{equation}
  \frac{b_{n, h} (x)^{4}}{\left( n \Sigma_{h} (x) \right)^{2}} \leq \frac{8
  \left( \E \left[ \left| K \left( \frac{X - x}{h} \right) \right|^{4} \right] +
  \left| \E \left[ K \left( \frac{X - x}{h} \right) \right] \right|^{4}
  \right)}{n \left( \E \left[ K \left( \frac{X - x}{h} \right)^{2} \right] - \E
  \left[ K \left( \frac{X - x}{h} \right) \right]^{2} \right)}.
  \label{eqn--bnh-var-ratio-bound}
\end{equation}
For any \(p \in \{1, 2, 4\}\),
\begin{equation}
  \E \left[ K \left( \frac{X - x}{h} \right)^{p} \right] = \int f (\xi) \left| K
  \left( \frac{\xi - x}{h} \right) \right|^{p} \; \mathrm{d} \xi = h^{d} \int f
  (x + u h) K (u)^{p} \; \mathrm{d} u.
  \label{eqn--kernel-local-moment}
\end{equation}
Plug \eqref{eqn--bnh-var-ratio-bound} into \eqref{eqn--kernel-local-moment} to
get
\begin{equation*}
  \frac{b_{n, h} (x)^{4}}{\left( n \Sigma_{h} (x) \right)^{2}} \leq \frac{8
  \left( h^{d} \int f (x + u h) K (u)^{4} \; \mathrm{d} u + \left( h^{d} \int f
  (x + u h) K (u) \; \mathrm{d} u \right)^{4} \right)}{n \left( h^{d} \int f (x
  + u h) K (u)^{2} \; \mathrm{d} u - \left( h^{d} \int f (x + u h) K (u) \;
  \mathrm{d} u \right)^{2} \right)^{2}}.
\end{equation*}
Then, since \(\int f (x + u h) K (u) \; \mathrm{d} u = f_{h} (x)\),
\begin{equation}
  \frac{b_{n, h} (x)^{4}}{\left( n \Sigma_{h} (x) \right)^{2}} \leq \frac{8
  \left( \int f (x + u h) K (u)^{4} \; \mathrm{d} u + h^{3 d} f_{h} (x)^{4}
  \right)}{n h^{d} \left( \int f (x + u h) K (u)^{2} \; \mathrm{d} u - h^{d}
  f_{h} (x)^{2} \right)^{2}}.
  \label{eqn--bnh-var-ratio-bound-1}
\end{equation}
By \Cref{thm--convl-kernel-eg-thin-tail}, since \(\lim_{n \to \infty} h_{n} =
0\),
\begin{equation*}
  \lim_{n \to \infty} \int f \left( x + u h_{n} \right) K (u)^{p} \; \mathrm{d}
  u = f (x) \int K (u) \; \mathrm{d} u \quad.
\end{equation*}
Furthermore, \Cref{asm--kernel} \ref{asm--kernel-unit-integral} implies that
\(\int K (u)^{p} \; \mathrm{d} u > 0\) for \(p \in \{2, 4\}\).
Combine this with the above displayed expression and
\eqref{eqn--bnh-var-ratio-bound-1} to get
\begin{equation*}
  \frac{b_{n, h_{n}} (x)^{4}}{\left( n \Sigma_{h_{n}} (x) \right)^{2}} \leq
  \frac{8}{n h_{n}} \cdot \frac{f (x) \int K (u)^{4} \; \mathrm{d} u + o (1) +
  h_{n}^{3 d} (f (x) + o (1))^{4}}{\left( f (x) \int K (u)^{2} \; \mathrm{d} u +
  o (1) - h_{n}^{d} (f (x) + o (1))^{2} \right)^{2}} \to 0,
\end{equation*}
since \(\lim_{n \to \infty} \max\left\{ h_{n}, 1 / \left( n h_{n}^{d} \right)
\right\} = 0\).
The claim in \eqref{eqn--kernel-sum-asymp-norm} now follows by
\eqref{eqn--liapunov-clt}.
\qed

\subsubsection{Proof of
\texorpdfstring{\Cref{thm--kde-asymp-norm-conv-var}}{Theorem
\ref{thm--kde-asymp-norm-conv-var}}}
\label{sec--prf--thm--kde-asymp-norm-conv-var}

The asymptotic standard normality result in
\eqref{eqn--kde-asymp-norm-conv-var}
follows from \eqref{eqn--kernel-sum-asymp-norm-1}, the variance consistency
result in \eqref{eqn--kde-asymp-norm-conv-var} and Slutsky's Theorem:
\begin{equation*}
  \frac{\widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x)}{\sqrt{\widehat{V}_{n, h_{n}}
  (x) / n}} = \sqrt{\frac{V_{h_{n}} (x)}{\widehat{V}_{n, h_{n}} (x)}} \cdot
  \frac{\widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x)}{\sqrt{V_{h_{n}} (x) / n}}
\rightsquigarrow \N (0, 1).
\end{equation*}
Hence, we focus on showing the variance consistency result in
\eqref{eqn--kde-asymp-norm-conv-var}, which is equivalent to showing
\begin{equation}
  \frac{\widehat{V}_{n, h_{n}} (x) - V_{h_{n}} (x)}{V_{h_{n}} (x)}
  \overset{\mathrm{p}}{\to} 0 \iff \widehat{V}_{n, h_{n}} (x) - V_{h_{n}} (x) =
  o_{\mathrm{p}} \left( V_{h_{n}} (x) \right).
  \label{eqn--kernel-sum-variance-consistency-1}
\end{equation}
To that end,
\begin{align*}
  \widehat{V}_{n, h} (x) - V_{h} (x) =
  & \, f_{h} (x)^{2} - \widehat{f}_{n, h} (x)^{2} + \frac{1}{n} \sum_{i = 1}^{n}
  \left\{ K_{h} \left( X_{i} - x \right)^{2} - \E \left[ K_{h} (X - x)^{2}
  \right] \right\} \\
  =
  & \, \left( f_{h} (x) + \widehat{f}_{n, h} (x) \right) \left( f_{h}
  (x) - \widehat{f}_{n, h} (x) \right) \\
  & + \frac{1}{n} \sum_{i = 1}^{n} \left\{ K_{h} \left( X_{i} - x \right)^{2} -
  \E \left[ K_{h} (X - x)^{2} \right] \right\} \\
  =
  & \, 2 f_{h} (x) \left( f_{h} (x) - \widehat{f}_{n, h} (x) \right) -
  \left( f_{h} (x) - \widehat{f}_{n, h} (x) \right)^{2} \\
  & + \frac{1}{n} \sum_{i = 1}^{n} \left\{ K_{h} \left( X_{i} - x \right)^{2} -
  \E \left[ K_{h} (X - x)^{2} \right] \right\}.
\end{align*}
Hence,
\begin{equation}
  \begin{split}
    \widehat{V}_{n, h} (x) - V_{h} (x) =
    & \, R_{n, h} (x) + T_{n, h} (x) \\
    \text{where} \quad
    R_{n, h} (x) =
    & \,  2 f_{h} (x) \left( f_{h} (x) - \widehat{f}_{n, h} (x) \right) -
    \left( f_{h} (x) - \widehat{f}_{n, h} (x) \right)^{2}, \\
    \text{and} \quad
    T_{n, h} (x) =
    & \, \frac{1}{n} \sum_{i = 1}^{n} \left\{ K_{h} \left( X_{i} - x \right)^{2}
    - \E \left[ K_{h} (X - x)^{2} \right] \right\}.
  \end{split}
\end{equation}
Therefore showing \eqref{eqn--kernel-sum-variance-consistency-1} is equivalent
to showing
\begin{equation}
  R_{n, h_{n}} (x) = o_{\mathrm{p}} \left( V_{h_{n}} (x) \right) \quad
  \text{and} \quad T_{n, h_{n}} (x) = o_{\mathrm{p}} \left( V_{h_{n}} (x)
  \right).
  \label{eqn--kernel-sum-variance-consistency-2}
\end{equation}

By \eqref{eqn--kernel-sum-asymp-norm-1},
\begin{equation*}
  \widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x) = O_{p} \left(
  \sqrt{\frac{V_{h_{n}} (x)}{n}} \right) = O_{p} \left( \frac{V_{h_{n}}
  (x)}{\sqrt{n V_{h_{n}} (x)}} \right),
\end{equation*}
and so,
\begin{equation}
  \widehat{f}_{n, h_{n}} (x) - f_{h_{n}} (x) = \frac{1}{\sqrt{n V_{h_{n}} (x)}}
  \cdot O_{p} \left( V_{h_{n}} (x) \right).
  \label{eqn--kernel-sum-variance-consistency-Rnh-1}
\end{equation}
Since
\begin{equation}
  V_{h} (x) = \frac{1}{h^{d}} \left( \int f (x + u h) K (u)^{2} \;
  \mathrm{d} u - h^{d} \left( \int f (x + u h) K (u) \; \mathrm{d} u \right)^{2}
  \right),
  \label{eqn--Vh-expanded-form}
\end{equation}
by \Cref{thm--convl-kernel-eg-thin-tail} it follows that
\begin{equation*}
  \frac{1}{n V_{h_{n}} (x)} = \frac{h_{n}^{d}}{n} \cdot \frac{1}{f (x) \int K
  (u)^{2} \; \mathrm{d} u + o (1)}.
\end{equation*}
Combine with this \eqref{eqn--kernel-sum-variance-consistency-Rnh-1}
to conclude that
\begin{equation}
  \widehat{f}_{n, h_{n}} - f_{h_{n}} (x) = o_{\mathrm{p}} \left( V_{h_{n}} (x)
  \right) \quad \text{and so} \quad R_{n, h_{n}} (x) = o_{\mathrm{p}} \left(
  V_{h_{n}} (x) \right).
  \label{eqn--kernel-sum-variance-consistency-Rnh}
\end{equation}

For the second part of \eqref{eqn--kernel-sum-variance-consistency-2}, we will
use Chebyshev's inequality.
Note that \eqref{eqn--Vh-expanded-form}
\begin{align*}
  T_{n, h_{n}} (x) =
  & \, O_{\mathrm{p}} \left( \sqrt{\E \left[ T_{n, h_{n}} (x)^{2} \right]}
  \right) = O_{\mathrm{p}} \left( V_{h} (x) \right) \frac{\sqrt{\E \left[ T_{n,
  h_{n}} (x)^{2} \right]}}{V_{h} (x)} \\
  =
  & \, O_{\mathrm{p}} \left( V_{h_{n}} (x) \right) \frac{h_{n}^{d}
  \sqrt{\E \left[ T_{n, h_{n}} (x)^{2} \right]}}{\left( \int f \left( x + u
  h_{n} \right) K (u)^{2} \; \mathrm{d} u - h_{n}^{d} \left( \int f \left( x + u
  h_{n} \right) K (u) \; \mathrm{d} u \right)^{2} \right)},
\end{align*}
and so, by \Cref{thm--convl-kernel-eg-thin-tail}
\begin{equation}
  T_{n, h_{n}} (x) = O_{\mathrm{p}} \left( V_{h_{n}} (x) \right)
  \frac{\sqrt{h_{n}^{2 d} \cdot \E \left[ T_{n, h_{n}} (x)^{2} \right]}}{f (x)
  \int K (u)^{2} \; \mathrm{d} u + o (1)}
  \label{eqn--kernel-sum-variance-consistency-Tnh-1}
\end{equation}
Now, since \(K_{h} \left( X_{i} - x \right)\) are iid,
\begin{align*}
  \E \left[ T_{n, h} (x)^{2} \right] =
  & \, \E \left[ \left( \frac{1}{n} \sum_{i = 1}^{n} \left\{ K_{h} \left( X_{i}
  - x \right)^{2} - \E \left[ K_{h} (X - x)^{2} \right] \right\} \right)^{2}
  \right] \\
  =
  & \, \frac{\E \left[ \left( K_{h} \left( X - x \right)^{2} - \E \left[
  K_{h} (X - x)^{2} \right] \right)^{2} \right]}{n} \\
  =
  & \, \frac{\E \left[ K_{h} \left( X - x \right)^{4} \right] - \E \left[ K_{h}
  (X - x)^{2} \right]^{2}}{n} \\
  =
  & \, \frac{\frac{1}{h^{3 d}} \int f (x + u h) K (u)^{4} \; \mathrm{d} u
  - \left( \frac{1}{h^{d}} \int f (x + u h) K (u)^{2} \; \mathrm{d} u
  \right)^{2}}{n}.
\end{align*}
Therefore,
\begin{equation*}
  h^{2 d} \E \left[ T_{n, h} (x)^{2} \right] =
  \frac{\int f (x + u h) K (u)^{4} \; \mathrm{d} u - h^{d} \left( \int
  f (x + u h) K (u)^{2} \; \mathrm{d} u \right)^{2}}{n h^{d}},
\end{equation*}
and so by \Cref{thm--convl-kernel-eg-thin-tail},
\begin{equation}
  h_{n}^{2 d} \E \left[ T_{n, h_{n}} (x)^{2} \right] = \frac{f (x) \int K
  (u)^{4} \; \mathrm{d} u + o (1)}{n h_{n}^{d}}.
  \label{eqn--kernel-sum-variance-consistency-Tnh-2}
\end{equation}
Combine \eqref{eqn--kernel-sum-variance-consistency-Tnh-2} with
\eqref{eqn--kernel-sum-variance-consistency-Tnh-1} to see that
\begin{equation}
  \begin{split}
    T_{n, h_{n}} (x) =
    & \, O_{\mathrm{p}} \left( V_{h_{n}} (x) \right)
    \frac{\sqrt{f (x) \int K (u)^{4} \; \mathrm{d} u + o (1)}}{f (x)
    \int K (u)^{2} \; \mathrm{d} u + o (1)} \cdot \frac{1}{n h_{n}^{d}}, \\
    \text{and so} \quad T_{n, h_{n}} (x) =
    & \, o_{\mathrm{p}} \left( V_{h_{n}} (x)
    \right).
  \end{split}
  \label{eqn--kernel-sum-variance-consistency-Tnh}
\end{equation}
Now, \eqref{eqn--kernel-sum-variance-consistency-2} follows from
\eqref{eqn--kernel-sum-variance-consistency-Rnh} and
\eqref{eqn--kernel-sum-variance-consistency-Tnh}.
\qed


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../note-kdre-inference"
%%% End:
